Foundation models are commonly deployed to Amazon SageMaker AI managed inference within a serving container. These serving containers automatically support the handling of HTTP requests, though some require reverse proxies in this deployment to support the HTTP serving requirements of a real-time endpoint on Amazon SageMaker AI managed inference. To date, the following LLM servers are supported:

--8<-- "ai-frameworks-table.md"

## vLLM
!!! todo "Under Construction"
    This section of the documentation is under construction.

## SGLang
!!! todo "Under Construction"
    This section of the documentation is under construction.

## TensorRT-LLM
!!! todo "Under Construction"
    This section of the documentation is under construction.

## LMI
!!! todo "Under Construction"
    This section of the documentation is under construction.

## DJL
!!! todo "Under Construction"
    This section of the documentation is under construction.