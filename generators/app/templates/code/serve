#!/bin/bash
# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

<% if (modelServer === 'vllm') { %>
echo "Starting vLLM server"
<% } else if (modelServer === 'sglang') { %>
echo "Starting SGLang server"
<% } else if (modelServer === 'tensorrt-llm') { %>
echo "Starting TensorRT-LLM server"
<% } else if (modelServer === 'lmi') { %>
echo "Starting LMI (Large Model Inference) server"
<% } else if (modelServer === 'djl') { %>
echo "Starting DJL Serving server"
<% } %>

<% if (modelServer === 'lmi' || modelServer === 'djl') { %>
# LMI/DJL containers use serving.properties for configuration
# The configuration file should be at /opt/ml/model/serving.properties
# DJL Serving will automatically start with this configuration

if [ ! -f /opt/ml/model/serving.properties ]; then
    echo "Error: serving.properties not found at /opt/ml/model/serving.properties"
    exit 1
fi

echo "Using configuration from /opt/ml/model/serving.properties"
cat /opt/ml/model/serving.properties

# DJL Serving is already configured in the base image
# This script is not typically needed for LMI/DJL as they have their own entrypoint
# But we provide it for consistency with other model servers
exit 0
<% } else { %>

# Initialize server arguments
<% if (modelServer === 'tensorrt-llm') { %>
# port 8081 for internal TensorRT-LLM server (nginx proxies on 8080)
SERVER_ARGS=(--host 0.0.0.0 --port 8081)
<% } else { %>
# port 8080 required by SageMaker: https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-container-response
SERVER_ARGS=(--host 0.0.0.0 --port 8080)
<% } %>

# Define the prefix for environment variables to look for
<% if (modelServer === 'vllm') { %>
PREFIX="VLLM_"
<% } else if (modelServer === 'sglang') { %>
PREFIX="SGLANG_"
<% } else if (modelServer === 'tensorrt-llm') { %>
PREFIX="TRTLLM_"
<% } %>
ARG_PREFIX="--"

# Define environment variables to exclude (internal variables set by base images)
<% if (modelServer === 'vllm') { %>
EXCLUDE_VARS=("VLLM_USAGE_SOURCE")
<% } else if (modelServer === 'sglang') { %>
EXCLUDE_VARS=()
<% } else if (modelServer === 'tensorrt-llm') { %>
# Exclude TRTLLM_MODEL as it's used as the positional MODEL argument
EXCLUDE_VARS=("TRTLLM_MODEL")
<% } %>

# Declare and populate array of matching environment variables
mapfile -t env_vars < <(env | grep "^${PREFIX}")

# Loop through the array and convert to command-line arguments
for var in "${env_vars[@]}"; do
    IFS='=' read -r key value <<< "$var"
    
    # Skip excluded variables
    skip=false
    for exclude in "${EXCLUDE_VARS[@]}"; do
        if [ "$key" = "$exclude" ]; then
            skip=true
            break
        fi
    done
    
    if [ "$skip" = true ]; then
        continue
    fi
    
    # Remove prefix, convert to lowercase, and replace underscores with dashes
    arg_name=$(echo "${key#"${PREFIX}"}" | tr '[:upper:]' '[:lower:]' | tr '_' '-')
    SERVER_ARGS+=("${ARG_PREFIX}${arg_name}")
    if [ -n "$value" ]; then
        SERVER_ARGS+=("$value")
    fi
done

echo "-------------------------------------------------------------------"
<% if (modelServer === 'vllm') { %>
echo "vLLM engine args: [${SERVER_ARGS[@]}]"
<% } else if (modelServer === 'sglang') { %>
echo "SGLang engine args: [${SERVER_ARGS[@]}]"
<% } else if (modelServer === 'tensorrt-llm') { %>
echo "TensorRT-LLM engine args: [${SERVER_ARGS[@]}]"
<% } %>
echo "-------------------------------------------------------------------"

# Pass the collected arguments to the main entrypoint
<% if (modelServer === 'vllm') { %>
exec python3 -m vllm.entrypoints.openai.api_server "${SERVER_ARGS[@]}"
<% } else if (modelServer === 'sglang') { %>
exec python3 -m sglang.launch_server "${SERVER_ARGS[@]}"
<% } else if (modelServer === 'tensorrt-llm') { %>
# TensorRT-LLM requires the model as a positional argument
# Syntax: trtllm-serve serve MODEL [OPTIONS]
if [ -z "$TRTLLM_MODEL" ]; then
    echo "Error: TRTLLM_MODEL environment variable is not set"
    exit 1
fi
exec trtllm-serve serve "$TRTLLM_MODEL" "${SERVER_ARGS[@]}"
<% } %>
<% } %>
